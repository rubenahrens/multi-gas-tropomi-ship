{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import harp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "import S3tools\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  # Adjusts path to include the parent directory, 'Code'\n",
    "# import HARP.harptools as ht\n",
    "import SentHub.catalog_tools as ct\n",
    "\n",
    "# get keys from Data\\keys.json\n",
    "with open('../../Data/keys.json') as f:\n",
    "    keys = json.load(f)\n",
    "    # Define your parameters\n",
    "    access_key = keys[\"s3\"][\"access_key\"]\n",
    "    secret_key = keys[\"s3\"][\"secret_key\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_operations(product: str, spatial_extent: list, filter_vars: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Returns a string of operations to be used in the harp.import_product function\n",
    "    This function needs to be changed if a user wants to use different operations or products.\n",
    "    The product names in harp are different from the product names in the S5P data. And need to be changed manually to ensure accuracy.\n",
    "    # product names: https://stcorp.github.io/harp/doc/html/ingestions/index.html#sentinel-5p-products\n",
    "    \"\"\"\n",
    "    lon_min, lat_min, lon_max, lat_max = spatial_extent\n",
    "    variables = {\n",
    "    \"HCHO\": [\"HCHO_slant_column_number_density\", \"tropospheric_HCHO_column_number_density_validity\"],\n",
    "    \"NO2\": [\"NO2_slant_column_number_density\", \"tropospheric_NO2_column_number_density_validity\"],\n",
    "    \"SO2\": [\"SO2_slant_column_number_density\", \"SO2_column_number_density_validity\"],\n",
    "    \"misc\": [\"datetime_start\", \"latitude\", \"longitude\", \"cloud_fraction\", \n",
    "             \"surface_meridional_wind_velocity\", \"latitude_bounds\", \"longitude_bounds\",\n",
    "             \"solar_zenith_angle\", \"sensor_zenith_angle\", \"solar_azimuth_angle\", \"sensor_azimuth_angle\"],\n",
    "    }\n",
    "    \n",
    "    ops = [f\"latitude>={lat_min}\",f\"latitude<={lat_max}\",f\"longitude>={lon_min}\",f\"longitude<={lon_max}\"]\n",
    "    \n",
    "    if filter_vars:\n",
    "        ops.append(f\"keep({','.join([x for x in variables[product]])})\")\n",
    "    \n",
    "    operations = \";\".join(ops)\n",
    "    return operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_reduce(files, products, spatial_extent, root : str = \"\", delete = False, filter_vars: bool = False):\n",
    "    \"\"\"\n",
    "    Merges and regrids multiple netcdf files into one file using harp\n",
    "    During the merge, unwanted variables are removed.\n",
    "    The \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : str\n",
    "        List of files to merge, equivalent to the urls of the files\n",
    "    products : str\n",
    "        List of products to merge, equivalent to the names of the products\n",
    "    root : str\n",
    "        Root directory of the input files, must end with a '/'\n",
    "    \"\"\"\n",
    "    \n",
    "    final_product = harp.Product()\n",
    "        \n",
    "    # load products' data\n",
    "    for file, product in zip(files, products):  \n",
    "        operation = simple_operations(product, spatial_extent)\n",
    "        gridded = harp.import_product(f\"{root}{file}\", operations=operation)\n",
    "        for variable in gridded:\n",
    "            if variable not in final_product:\n",
    "                final_product[variable] = gridded[variable]\n",
    "\n",
    "    # add misc variables\n",
    "    if filter_vars:\n",
    "        operation = simple_operations(\"misc\", spatial_extent)\n",
    "        gridded = harp.import_product(f\"{root}{files[0]}\", operations=operation)\n",
    "        for variable in gridded:\n",
    "            final_product[variable] = gridded[variable]\n",
    "\n",
    "    # add source product to the final product for reference, it will automatically extract the file name\n",
    "    final_product.source_product = files[-1]\n",
    "\n",
    "    # get orbit id from the file name\n",
    "    orbit_id = ct.parse_s5p_filename(files[0].split(\"/\")[-2], \"orbit_id\")\n",
    "    date = files[0].split(\"/\")[3:6]\n",
    "\n",
    "    # make new filename out of the old filename\n",
    "    path = f\"{root}Sentinel-5P/TROPOMI/L3__Merged_/{date[0]}/{date[1]}/{date[2]}/{orbit_id}.nc\"\n",
    "\n",
    "    # save the final product\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    harp.export_product(final_product, path)\n",
    "\n",
    "    # delete the original files and the folders L2__HCHO__, L2__NO2__, L2__SO2__\n",
    "    if delete:\n",
    "        for file in files:\n",
    "            try:\n",
    "                file_dir = '/'.join(file.split(\"/\")[:-1])\n",
    "                if os.path.exists(f\"{root}{file_dir}\"):\n",
    "                    os.remove(f\"{root}{file}\")\n",
    "                os.removedirs(f\"{root}{file_dir}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging tends to be very fast, most of the time is spent downloading\n",
    "\n",
    "`\n",
    "S5P_RPRO_L2__NO2____20190828T121434_20190828T135604_09706_03_020400_20221106T095252.nc:   1%|          | 8/1059 [05:37<12:07:40, 41.54s/orbit]\n",
    "`\n",
    "\n",
    "Due to the long runtime, api side error can occur, simply restarting the program would fix these\n",
    "S5P_RPRO_L2__HCHO___20190804T080247_20190804T094416_09363_03_020401_20230122T105828 wont download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Mediterranean...: 100%|██████████| 10/10 [00:01<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Biscay Bay...: 100%|██████████| 10/10 [00:01<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Arabian Sea...: 100%|██████████| 10/10 [00:01<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Bengal Bay...: 100%|██████████| 10/10 [00:01<00:00,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ROOT = \"../../Data/\"\n",
    "\n",
    "# open ../../Data/aoi.json as dict\n",
    "with open(f\"{ROOT}aoi.json\", \"r\") as f:\n",
    "  aoi = json.load(f)\n",
    "\n",
    "# iterate over the areas of interest\n",
    "for aoi_name, area_of_interest in aoi.items():\n",
    "  # check if the download list already exists\n",
    "  if not os.path.exists(f\"{ROOT}Catalog/{aoi_name}/download_list.csv\"):\n",
    "    print(f\"Download list for {aoi_name} doesn't exists, skipping...\")\n",
    "    continue\n",
    "\n",
    "  target_path = f\"{ROOT}EODATA/{aoi_name}/\"\n",
    "  \n",
    "  # retrieve a list of .nc files in the merged folder\n",
    "  merged_files = glob(f\"{target_path}Sentinel-5P/TROPOMI/L3__Merged_/*/*/*/*.nc\")\n",
    "  # create a list of orbit ids from the merged files, this might work on windows only\n",
    "  merged_orbits = [int(file.split(\"\\\\\")[-1][:-3]) for file in merged_files]\n",
    "  \n",
    "  # path to the product to download\n",
    "  product_df = pd.read_csv(f\"{ROOT}Catalog/{aoi_name}/download_list.csv\")\n",
    "  \n",
    "  # remove duplicate rows\n",
    "  product_df.drop_duplicates(subset=[\"s3_url\"], inplace=True)\n",
    "  \n",
    "  # get the unique orbit ids\n",
    "  unique_orbits = product_df[\"sat:absolute_orbit\"].unique().tolist()\n",
    "  # remove the orbits that already have a merged file\n",
    "  unique_orbits = [orbit for orbit in unique_orbits if orbit not in merged_orbits]\n",
    "  \n",
    "  if len(unique_orbits) == 0:\n",
    "    # fake progress bar to simulate processing\n",
    "    with tqdm(total=10, desc=f\"Processing {aoi_name}...\") as pbar:  # Create the progress bar\n",
    "        for _ in range(10):  # A simple loop\n",
    "            time.sleep(0.1)  # Simulate processing delay\n",
    "            pbar.update(1)   # Update the progress bar\n",
    "    print()\n",
    "    continue\n",
    "  \n",
    "  # iterate over unique orbit ids. There should be 3 products for each orbit\n",
    "  loop = tqdm(unique_orbits, desc=f\"Downloading {aoi_name} orbits for box {area_of_interest}...\\n\", \n",
    "                       unit=\"orbit\", colour=\"green\", dynamic_ncols=True)\n",
    "  for orbit_id in loop:\n",
    "      \n",
    "      # get the first product for each orbit\n",
    "      orbit_df = product_df[product_df[\"sat:absolute_orbit\"] == orbit_id]\n",
    "      files = orbit_df[\"s3_url\"].to_list()\n",
    "      \n",
    "      # download the products\n",
    "      for url in files:\n",
    "          loop.set_description_str(f\"{url.split('/')[-1]}\")\n",
    "          S3tools.download(url, target_path)\n",
    "      \n",
    "      products = orbit_df[\"s5p:type\"].to_list()\n",
    "      loop.set_description_str(f\"Merging {products} for orbit {orbit_id}\")\n",
    "      merged = merge_reduce(files, products, area_of_interest, target_path, delete=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
