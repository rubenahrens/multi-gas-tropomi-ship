{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catalog_tools as ct\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "TARGET = \"../../Data/Catalog/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that filtering on the API side is not efficient\n",
    "\n",
    "The maximimum RPRO products in one day is 58, if we don't filter on product, it takes 10 minutes to retrieve 3 years of products with the catalog API. If products are filtered it takes 2m44s to retrieve one year, so 8m15 for 3 years. Filtering is not worth it time wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orbit_week(date0, date1, area_of_interest, products=[\"HCHO\", \"NO2\", \"SO2\"], min_coverage=1, collection_id=\"03\"):\n",
    "  \"\"\"\n",
    "  Get the orbit for a given date and area of interest\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  date: str\n",
    "  Date in the format \"YYYY-MM-DD\"\n",
    "  products: list\n",
    "  List of products to consider\n",
    "  min_coverage: float\n",
    "  Minimum coverage of the area of interest by the orbit\n",
    "  collection_id: str\n",
    "  Collection id of the orbit\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  output_files: list\n",
    "  List of orbit filenames\n",
    "  \"\"\"\n",
    "  # add features in flat list\n",
    "  features = []\n",
    "  for product in products:\n",
    "    response = ct.request(date0, area_of_interest, product, date1=date1)[\"features\"]\n",
    "    assert len(response) < 100, \"Went over catalog API limit, please adjust date range or area of interest.\"\n",
    "    features.extend(response)\n",
    "\n",
    "  response_df = gpd.GeoDataFrame.from_features(features, \n",
    "    columns=[\"geometry\", \"datetime\", \"s5p:type\", \"sat:absolute_orbit\", \"collection_id\"])\n",
    "\n",
    "  # add column with s3 url\n",
    "  response_df[\"s3_url\"] = [f[\"assets\"][\"data\"][\"href\"][12:] for f in features]\n",
    "  \n",
    "  # remove rows with duplicate s3 url\n",
    "  response_df.drop_duplicates(subset=\"s3_url\", inplace=True)\n",
    "  \n",
    "  # add column with collection id\n",
    "  response_df[\"collection_id\"] = [f[\"id\"][58:60] for f in features]\n",
    "  \n",
    "  # filter by collection id\n",
    "  response_df = response_df[response_df[\"collection_id\"] == collection_id]\n",
    "  \n",
    "  orbit_df_list = []\n",
    "  # remove orbits that do not have all the products\n",
    "  # iterate over unique orbits\n",
    "  for orbit in response_df[\"sat:absolute_orbit\"].unique():\n",
    "    orbit_df = response_df[response_df[\"sat:absolute_orbit\"] == orbit].copy()\n",
    "    \n",
    "    # check if all products are available\n",
    "    products_orbit = orbit_df[\"s5p:type\"].unique()\n",
    "    if set(products).difference(products_orbit):\n",
    "      continue\n",
    "    \n",
    "    # calculate coverage and ignore if below threshold\n",
    "    orbit_df.loc[:, \"coverage\"] = orbit_df.apply(lambda row: ct.calculate_coverage(row[\"geometry\"], area_of_interest), axis=1)\n",
    "    if orbit_df['coverage'].min() < min_coverage:\n",
    "      continue\n",
    "        \n",
    "    orbit_df_list.append(orbit_df)\n",
    "    \n",
    "  if not orbit_df_list:\n",
    "    # return empty dataframe if no orbits are found\n",
    "    return pd.DataFrame()\n",
    "  \n",
    "  response_df = pd.concat(orbit_df_list)        \n",
    "  response_df.set_index([\"sat:absolute_orbit\", \"s5p:type\"], inplace=True)\n",
    "  response_df.sort_index(inplace=True)\n",
    "  return response_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 5 minutes per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Mediterranean file list for box [14, 33.2, 19.3, 38]:   0%|\u001b[32m          \u001b[0m| 0/111 [00:00<?, ?2 weeks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Mediterranean file list for box [14, 33.2, 19.3, 38]: 100%|\u001b[32m██████████\u001b[0m| 111/111 [03:11<00:00,  1.72s/2 weeks]\n",
      "Downloading Biscay Bay file list for box [-10, 45, -6, 47]: 100%|\u001b[32m██████████\u001b[0m| 111/111 [03:08<00:00,  1.70s/2 weeks]\n",
      "Downloading Arabian Sea file list for box [59, 5, 68.5, 18]: 100%|\u001b[32m██████████\u001b[0m| 111/111 [04:19<00:00,  2.34s/2 weeks]\n",
      "Downloading Bengal Bay file list for box [88, 2, 92, 8]: 100%|\u001b[32m██████████\u001b[0m| 111/111 [04:49<00:00,  2.61s/2 weeks]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# open ../../Data/aoi.json as dict\n",
    "with open(\"../../Data/aoi.json\", \"r\") as f:\n",
    "  aoi = json.load(f)\n",
    "\n",
    "# iterate over the areas of interest\n",
    "for aoi_name, area_of_interest in aoi.items():\n",
    "  # check if the download list already exists\n",
    "  if os.path.exists(f\"{TARGET}/{aoi_name}/download_list.csv\"):\n",
    "    print(f\"Download list for {aoi_name} already exists, skipping...\")\n",
    "    continue\n",
    "  \n",
    "  # info on date range: https://www.temis.nl/airpollution//no2col/tropomi_no2_data_versions.php\n",
    "  date0 = \"2018-05-01\"\n",
    "  date_min1 = \"2022-07-25\"\n",
    "  download_list = []\n",
    "  # empty_count = 0\n",
    "  for date1 in tqdm(pd.date_range(start=date0, end=date_min1, freq=\"2W\"), \n",
    "                    desc=f\"Downloading {aoi_name} file list for box {area_of_interest}\", unit=\"2 weeks\", colour=\"green\"):\n",
    "    date1 = date1.date()\n",
    "    # Get the dataframe for the specific date range, you can change the products and collection_id\n",
    "    response = get_orbit_week(date0, date1, area_of_interest)\n",
    "    if not response.empty:\n",
    "      download_list.append(response)\n",
    "    # elif empty_count > 3:\n",
    "    #   print(f\"No data found for {aoi_name} for the last 2 months, skipping...\")\n",
    "    #   break\n",
    "    # else:\n",
    "    #   empty_count += 1\n",
    "    date0 = date1\n",
    "    \n",
    "  if not download_list:\n",
    "    print(f\"No data found for {aoi_name}, skipping...\")\n",
    "    continue\n",
    "  download_list = pd.concat(download_list)\n",
    "\n",
    "  # remove geometry and collection id column\n",
    "  download_list.drop(columns=[\"geometry\", \"collection_id\"], inplace=True)\n",
    "  \n",
    "  # remove duplicates \n",
    "  download_list.drop_duplicates(subset=[\"s3_url\"], inplace=True)\n",
    "\n",
    "  # save to csv\n",
    "  if not os.path.exists(f\"{TARGET}/{aoi_name}\"):\n",
    "    os.makedirs(f\"{TARGET}/{aoi_name}\")\n",
    "  download_list.to_csv(f\"{TARGET}/{aoi_name}/download_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "\n",
    "| geometry | datetime | s5p:type | sat:absolute_orbit | collection_id | s3_url | coverage |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 14 MULTIPOLYGON (((-180.00000 -90.00000, 180.0000... | 2019-11-28T11:40:16Z | HCHO | 11011 | 3 | Sentinel-5P/TROPOMI/L2__HCHO__/2019/11/28/S5P_... | 1.000000 |\n",
    "| 35 MULTIPOLYGON (((-180.00000 -90.00000, 180.0000... | 2019-11-28T11:40:16Z | NO2 | 11011 | 3 | Sentinel-5P/TROPOMI/L2__NO2___/2019/11/28/S5P_... | 0.999997 |\n",
    "| 56 MULTIPOLYGON (((-180.00000 -90.00000, 180.0000... | 2019-11-28T11:40:16Z | SO2 | 11011 | 3 | Sentinel-5P/TROPOMI/L2__SO2___/2019/11/28/S5P_... | 1.000000 |\n",
    "\n",
    "Absolute orbits can have ever so slightly different geometries, by filtering on coverage per row, we will end up with some files that have missing products. This is an issue with Catalog_1.0 and EODATA_1.0, but is fixed in 1.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
